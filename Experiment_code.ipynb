{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baIP6pM9lxa4"
   },
   "source": [
    "Running record of prior work:\n",
    "*   https://arxiv.org/pdf/2307.10236\n",
    "\n",
    "\n",
    "Self-reported uncertainty:\n",
    "*   https://aclanthology.org/2024.acl-long.283.pdf - primary inspiration for our method\n",
    "*   https://arxiv.org/pdf/2207.05221 - Earlier method, iterated on by 2024.acl-long.283\n",
    "*   https://aclanthology.org/2024.emnlp-main.299.pdf - only applies to long form (multi-sentence) generation. Also requires multiple long-form responses to compute the uncertainty for a single response.\n",
    "*   https://aclanthology.org/2024.emnlp-main.1205.pdf - sentence level, requires multisampling, SAR scores are on shaky theoretical grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLciJBPIH-fs"
   },
   "source": [
    "# Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dVdSWXjUG8NG"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import ast\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import time\n",
    "import accelerate\n",
    "import pickle\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import transformers as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nG1Fggj5KTXi"
   },
   "outputs": [],
   "source": [
    "my_token = 'INSERT JUPYTER NOTEBOOK TOKEN HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GLvvoOnIn5P"
   },
   "source": [
    "# Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieGNCsl_HNDH"
   },
   "outputs": [],
   "source": [
    "def load_data(filenames):\n",
    "    ctx = filenames['Question']\n",
    "    cmplts = filenames['Answers']\n",
    "\n",
    "    contexts = []\n",
    "    for i in ctx:\n",
    "      contexts.append(i)\n",
    "\n",
    "    completions = []\n",
    "    for i in cmplts:\n",
    "      completions.append(i)\n",
    "\n",
    "    return contexts, completions\n",
    "\n",
    "\n",
    "def load_model(model_name, device='cuda', token=None, verbose=False):\n",
    "    config = tf.AutoConfig.from_pretrained(model_name, token=my_token)\n",
    "    tokenizer = tf.AutoTokenizer.from_pretrained(model_name, token=my_token)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    try:\n",
    "        transformer = tf.AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                   config=config,\n",
    "                                                   token=my_token,\n",
    "                                                   resume_download=True,\n",
    "                                                   low_cpu_mem_usage=True,\n",
    "                                                   device_map=\"auto\")\n",
    "        if verbose:\n",
    "            print(f'Successfully loaded model: {model_name}')\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f'[ERROR] Failed to load model: {model_name}')\n",
    "        raise e\n",
    "\n",
    "    transformer.pop_ready = False\n",
    "\n",
    "    return transformer, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8k4O53GI33o"
   },
   "source": [
    "# Uncertainty Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wKddiSXJFwN"
   },
   "outputs": [],
   "source": [
    "def next_token_pdf(model, tokenizer, ctx):\n",
    "    tokenized = tokenizer(ctx, return_tensors='pt', padding=True, add_special_tokens=False).to(model.device.type)\n",
    "    outputs = model(**tokenized)\n",
    "\n",
    "    return torch.nn.LogSoftmax(dim=-1)(outputs.logits[0, -1, :]).exp().detach()\n",
    "\n",
    "def cloze_test(model, tokenizer, pdf, tok_cmplts):\n",
    "    probs = pdf[tok_cmplts['input_ids'][:,0]]\n",
    "\n",
    "    return np.argmax(probs.detach().cpu().numpy())\n",
    "\n",
    "def unc_self_report(model, tokenizer, ctx, cmplts, output_pdf, choice, choice_txt):\n",
    "    '''\n",
    "      result is a list of shape (cmplts_count, 2)\n",
    "\n",
    "      [\n",
    "        answer choice 1 results = [prob(best), prob(worst)],\n",
    "        answer choice 2 results = [prob(best), prob(worst)],\n",
    "        ...\n",
    "      ]\n",
    "    '''\n",
    "    choice_uqs = []\n",
    "    unc_cmplts = ['best', 'worst']\n",
    "    uq_cmplts_tok = tokenizer(unc_cmplts, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "    for c in cmplts:\n",
    "        uncertainty_prompt = f'{c}\\nThis answer is '\n",
    "        uq_pdf = next_token_pdf(model, tokenizer, ctx + uncertainty_prompt)\n",
    "        choice_uqs.append(list(uq_pdf[uq_cmplts_tok['input_ids'][:,0]].detach().cpu().numpy()))\n",
    "    return choice_uqs\n",
    "\n",
    "def unc_frequency(model, tokenizer, ctx, cmplts, output_pdf, choice, choice_txt, trials=100):\n",
    "    tok_cmplts = tokenizer(cmplts, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "    choice_uqs = list(output_pdf[tok_cmplts['input_ids'][:,0]].detach().cpu().numpy() * trials)\n",
    "\n",
    "    return choice_uqs\n",
    "\n",
    "def unc_cred_int(model, tokenizer, ctx, cmplts, output_pdf, choice, choice_txt, p=0.95):\n",
    "    sorted_probs, indices = torch.sort(output_pdf, dim=-1, descending=True)\n",
    "    cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    nucleus = torch.count_nonzero(cum_sum_probs < p)\n",
    "\n",
    "    return nucleus.item()\n",
    "\n",
    "def unc_entropy(model, tokenizer, ctx, cmplts, output_pdf, choice, choice_txt, choices_only=False):\n",
    "    if choices_only:\n",
    "        tok_cmplts = tokenizer(cmplts, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "        choices_pdf = output_pdf[tok_cmplts['input_ids'][:,0]]\n",
    "        return np.sum(choices_pdf.detach().cpu().numpy() * np.log(choices_pdf.detach().cpu().numpy()))\n",
    "    else:\n",
    "        return np.sum(output_pdf.detach().cpu().numpy() * np.log(output_pdf.detach().cpu().numpy()))\n",
    "\n",
    "def unc_choice_entropy(model, tokenizer, ctx, cmplts, output_pdf, choice, choice_txt):\n",
    "    return unc_entropy(model, tokenizer, ctx, cmplts, output_pdf, choice, choice_txt, choices_only=True)\n",
    "\n",
    "def unc_topk_entropy(model, tokenizer, ctx, cmplts, output_pdf, choice, choice_txt, k=10):\n",
    "    topk = torch.topk(output_pdf, k)[0].detach().cpu().numpy()\n",
    "\n",
    "    return np.sum(topk * np.log(topk))\n",
    "\n",
    "def get_uncertainty(model, tokenizer, ctx, cmplts, uncertainty_type, output_pdf, choice, choice_txt, **kwargs):\n",
    "    return unc_funcs[uncertainty_type](model, tokenizer, ctx, cmplts, output_pdf, choice, choice_txt, **kwargs)\n",
    "\n",
    "def next_token_pdf_alt(model, tokenizer, ctx):\n",
    "    tokenized = tokenizer(ctx, return_tensors='pt', padding=True, add_special_tokens=False).to(model.device.type)\n",
    "    outputs = model(**tokenized)\n",
    "\n",
    "    return torch.nn.LogSoftmax(dim=-1)(outputs.logits[:, -1, :]).exp().detach()\n",
    "\n",
    "unc_funcs = {\n",
    "    'self-report' : unc_self_report,\n",
    "    'freq'        : unc_frequency,\n",
    "    'top-p'       : unc_cred_int,\n",
    "    'entropy'     : unc_entropy,\n",
    "    'c-ent'       : unc_choice_entropy,\n",
    "    'top-k-ent'   : unc_topk_entropy,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir3HK2scJIzb"
   },
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLWziDYsJMU_"
   },
   "outputs": [],
   "source": [
    "def prepare_model_for_pop(model):\n",
    "    pop.DropoutUtils.add_new_dropout_layers(model)\n",
    "    pop.DropoutUtils.convert_dropouts(model)\n",
    "    pop.DropoutUtils.activate_mc_dropout(model, activate=True, random=0.1)\n",
    "\n",
    "    model.pop_ready = True\n",
    "\n",
    "def run_experiment(model,\n",
    "                   tokenizer,\n",
    "                   data_locs,\n",
    "                   output_dir,\n",
    "                   causal=True,\n",
    "                   device='cuda',\n",
    "                   token=None,\n",
    "                   early_exit=False,\n",
    "                   verbose=False,\n",
    "                   **kwargs):\n",
    "    #copy original table\n",
    "    df = pd.read_pickle(data_locs)\n",
    "\n",
    "    if early_exit:\n",
    "        df = df.head(3)\n",
    "\n",
    "    #add model output and pdf columns to df\n",
    "    df[\"model output\"] = None\n",
    "    df[\"pdf\"] = None\n",
    "    \n",
    "    #create output file. if it exists, append with new number\n",
    "    version = 1\n",
    "    while (os.path.exists(output_dir)):\n",
    "        if '(' in output_dir:\n",
    "            output_dir = output_dir[:1] + str(version + 1) + output_dir[2:]\n",
    "        else:\n",
    "            output_dir = '(1)' + output_dir\n",
    "        version += 1\n",
    "    df.to_pickle(output_dir)\n",
    "    \n",
    "    #instructional content to be prepended to contexts\n",
    "    instr_str = \"Following is a question and a selection of answer choices. Provide the label for the answer with which you most agree.\\nQuestion: \"\n",
    "    query_str = \"\\nAnswer: \"\n",
    "    index = 0\n",
    "    \n",
    "    #make cmplt context lists\n",
    "    quest_fmt = \"\\n {}. {}\"\n",
    "    contexts, completions = load_data(df)\n",
    "    indices = 0\n",
    "    for cmplt, ctx in zip(completions, contexts):\n",
    "        cmplt = str(cmplt).split(\",\")\n",
    "        for index, j in enumerate(cmplt):\n",
    "            letter = chr(index + 65)\n",
    "            ctx = ctx + quest_fmt.format(letter, j)\n",
    "            cmplt[index] = letter\n",
    "        completions[indices] = cmplt\n",
    "        contexts[indices] = instr_str + ctx + query_str\n",
    "        indices += 1\n",
    "        \n",
    "    index = 0\n",
    "    \n",
    "    #\n",
    "    for ctx, cmplt in zip(contexts, completions):\n",
    "        \n",
    "        tok_cmplts = tokenizer(cmplt, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "        output_pdf = next_token_pdf(model, tokenizer, ctx)\n",
    "        choice = cloze_test(model, tokenizer, output_pdf, tok_cmplts)\n",
    "        choice_txt = cmplt[choice]\n",
    "\n",
    "        #append answer to answer column in df\n",
    "        df.at[index, 'model output'] = choice_txt\n",
    "        \n",
    "        #append pdf to pdf column in df\n",
    "        df.at[index, 'pdf'] = output_pdf\n",
    "        \n",
    "        #save file every line\n",
    "        df.to_pickle(output_dir)\n",
    "\n",
    "        percent = round((index / 3000) * 100, 3)\n",
    "        print(f'\\tsaved {percent}% of lines to {output_dir}', end='\\r')\n",
    "        \n",
    "        index += 1\n",
    "        \n",
    "    #save df as pickle\n",
    "    df.to_pickle(output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)\n",
    " \n",
    "def load_df(filename) -> pd.DataFrame:\n",
    "    with open(filename, 'rb') as f:\n",
    "        df = CPU_Unpickler(f).load()\n",
    "   \n",
    "    return df\n",
    "   \n",
    "def norm_ent(probs):\n",
    "    return scipy.stats.entropy(probs) / np.log(len(probs))\n",
    " \n",
    "def get_top_p(sorted_probs, p):\n",
    "    cum_sum = np.cumsum(sorted_probs)\n",
    "    k = np.searchsorted(cum_sum, p, side='right')\n",
    "    return sorted_probs[:k]\n",
    " \n",
    "def get_metrics(df, dataset_mode, model_name, hf_token):\n",
    "    '''\n",
    "    dataset_mode:\n",
    "        0 = mmlu, 1 = roper\n",
    " \n",
    "    Implemented metrics:\n",
    "        top-k:\n",
    "            k = 5, 10, 25, 50, 100\n",
    "            entropy\n",
    "        top-p:\n",
    "            p = 0.95, 0.9, 0.75, 0.5\n",
    "            entropy\n",
    "            size\n",
    "        total entropy\n",
    "        choices:\n",
    "            entropy\n",
    "            raw probs\n",
    "        non-entropy:\n",
    "            top-1 prob\n",
    "    '''\n",
    "    metric_df = pd.DataFrame()\n",
    " \n",
    "    # choices first (before sorting)\n",
    "    tokenizer = tf.AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    choices = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    choices = tokenizer(choices, return_tensors='np', padding=False, add_special_tokens=False)['input_ids'][:,0]\n",
    " \n",
    "#     metrics_df['pdf'] = df.apply(lambda x: x['pdf'].to_device('cuda'))\n",
    "\n",
    "    if dataset_mode == 0:\n",
    "        metric_df['choice-probs'] = df.apply(lambda x: x['pdf'][choices[:4]].tolist(), axis=1)\n",
    "    elif dataset_mode == 1:\n",
    "        metric_df['choice-probs'] = df.apply(lambda x: x['pdf'][choices[:len(x['Answer_Ratios'])]].tolist(), axis=1)\n",
    "    elif dataset_mode == 2:\n",
    "        metric_df['choice-probs'] = df.apply(lambda x: x['pdf'][choices[:len(x['Answers'].split(','))]].tolist(), axis=1)\n",
    "    \n",
    "    metric_df['choice-ent'] = metric_df['choice-probs'].apply(lambda x: norm_ent(x))\n",
    " \n",
    "    #sort tensor\n",
    "    sorted_df = pd.DataFrame()\n",
    "    sorted_df['pdf'] = df['pdf'].apply(lambda x: np.sort(x)[::-1])\n",
    " \n",
    "    # non-entropy\n",
    "    metric_df['top-1-abs'] = sorted_df['pdf'].apply(lambda x: x[0])\n",
    " \n",
    "    # total-entropy\n",
    "    metric_df['total-ent'] = sorted_df['pdf'].apply(lambda x: norm_ent(x))\n",
    " \n",
    " \n",
    "    # top-k\n",
    "    for k in [5, 10, 25, 50, 100]:\n",
    "        metric_df[f'top-k-ent-{k}'] = sorted_df['pdf'].apply(lambda x: norm_ent(x[:k]))\n",
    " \n",
    "    # top-p\n",
    "    for p in [0.95, 0.9, 0.75, 0.5]:\n",
    "        top_p = sorted_df['pdf'].apply(lambda x: get_top_p(x, p))\n",
    "        metric_df[f'top-p-len-{p}'] = top_p.apply(lambda x: len(x))\n",
    "        metric_df[f'top-p-ent-{p}'] = top_p.apply(lambda x:\n",
    "                                                0 if len(x) == 1 else     # no uncertainty when only one element qualifies\n",
    "                                                1 if len(x) == 0 else     # complete uncertainty when no elements qualify\n",
    "                                                norm_ent(x)\n",
    "                                                )\n",
    " \n",
    "    df = pd.concat([df, metric_df], axis=1)\n",
    "    df = df.drop('pdf', axis=1)\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDrYNUbYSK-j"
   },
   "source": [
    "llama-3.1 1,3,8\n",
    "solar\n",
    "mistral\n",
    "mistral 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbVASoiGLgBS"
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "     'meta-llama/Llama-3.2-1B',\n",
    "     'meta-llama/Llama-3.2-3B',\n",
    "     'meta-llama/Llama-3.1-8B',\n",
    "     'mistralai/Mistral-7B-v0.1',\n",
    "     'mistralai/Mistral-7B-v0.3',\n",
    "     'meta-llama/Llama-3.2-1B-Instruct',\n",
    "     'meta-llama/Llama-3.2-3B-Instruct',\n",
    "     'meta-llama/Llama-3.1-8B-Instruct',\n",
    "     'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "     'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "]\n",
    "iRoperPickles = [\n",
    "    '(1)meta-llama-Llama-3.2-1B.pkl', \n",
    "    'meta-llama-Llama-3.2-3B.pkl',\n",
    "    'meta-llama-Llama-3.1-8B.pkl', \n",
    "    'mistralai-Mistral-7B-v0.1.pkl',\n",
    "    'mistralai-Mistral-7B-v0.3.pkl', \n",
    "    'meta-llama-Llama-3.2-1B-Instruct.pkl', \n",
    "    'meta-llama-Llama-3.2-3B-Instruct.pkl',\n",
    "    'meta-llama-Llama-3.1-8B-Instruct.pkl', \n",
    "    'mistralai-Mistral-7B-Instruct-v0.1.pkl', \n",
    "    'mistralai-Mistral-7B-Instruct-v0.3.pkl', \n",
    "]\n",
    "\n",
    "mmlu_pickles = [\n",
    "    'MMLU/meta-llama-Llama-3.2-1B.pkl',\n",
    "    'MMLU/meta-llama-Llama-3.2-3B.pkl',\n",
    "    'MMLU/meta-llama-Llama-3.1-8B.pkl', \n",
    "    'MMLU/mistralai-Mistral-7B-v0.1.pkl',\n",
    "    'MMLU/mistralai-Mistral-7B-v0.3.pkl',\n",
    "    'MMLU/meta-llama-Llama-3.2-1B-Instruct.pkl', \n",
    "    'MMLU/meta-llama-Llama-3.2-3B-Instruct.pkl',\n",
    "    'MMLU/meta-llama-Llama-3.1-8B-Instruct.pkl', \n",
    "    'MMLU/mistralai-Mistral-7B-Instruct-v0.1.pkl', \n",
    "    'MMLU/mistralai-Mistral-7B-Instruct-v0.3.pkl',\n",
    "]\n",
    "\n",
    "humanPickles = [\n",
    "    'Human Data/meta-llama-Llama-3.2-1B (1).pkl',\n",
    "    'Human Data/meta-llama-Llama-3.2-3B (1).pkl',\n",
    "    'Human Data/meta-llama-Llama-3.1-8B (1).pkl',\n",
    "    'Human Data/mistralai-Mistral-7B-v0.1 (1).pkl',\n",
    "    'Human Data/mistralai-Mistral-7B-v0.3 (1).pkl',\n",
    "    'Human Data/meta-llama-Llama-3.2-1B-Instruct (1).pkl',\n",
    "    'Human Data/meta-llama-Llama-3.2-3B-Instruct (1).pkl',\n",
    "    'Human Data/meta-llama-Llama-3.1-8B-Instruct (1).pkl',\n",
    "    'Human Data/mistralai-Mistral-7B-Instruct-v0.1 (1).pkl',\n",
    "    'Human Data/mistralai-Mistral-7B-Instruct-v0.3 (1).pkl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:11,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "for m_name, pkl_file in tqdm.tqdm(zip(models, humanPickles)):\n",
    "    # 0 = mmlu, 1 = roper\n",
    "    d_mode = 2\n",
    "\n",
    "    init_df = load_df(pkl_file)\n",
    "\n",
    "    if d_mode == 1:\n",
    "        # init_df['model output'] = pd.concat([init_df['model output'][2:], init_df['model output'][:2]], axis=-1)\n",
    "        # init_df['pdf']          = pd.concat([init_df['pdf'][2:], init_df['pdf'][:2]], axis=-1)\n",
    "        init_df['model output'] = init_df['model output'].shift(-1)\n",
    "        init_df['pdf'] = init_df['pdf'].shift(-1)\n",
    "\n",
    "        init_df = init_df[:-2]\n",
    "\n",
    "    elif d_mode == 2:\n",
    "        init_df['model output'] = init_df['model output'].shift(-1)\n",
    "        init_df['pdf'] = init_df['pdf'].shift(-1)\n",
    "\n",
    "        init_df = init_df[:-1]\n",
    "\n",
    "    dfMetrics = get_metrics(init_df, d_mode, m_name, my_token)\n",
    "    dfMetrics.to_csv(f'{pkl_file}_human.csv')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_name, pkl_file in zip(models, mmlu_pickles):\n",
    "    # 0 = mmlu, 1 = roper\n",
    "    d_mode = 0\n",
    "\n",
    "    init_df = load_df(pkl_file)\n",
    "\n",
    "    if d_mode == 1:\n",
    "        # init_df['model output'] = pd.concat([init_df['model output'][2:], init_df['model output'][:2]], axis=-1)\n",
    "        # init_df['pdf']          = pd.concat([init_df['pdf'][2:], init_df['pdf'][:2]], axis=-1)\n",
    "        init_df['model output'] = init_df['model output'].shift(-2)\n",
    "        init_df['pdf'] = init_df['pdf'].shift(-2)\n",
    "\n",
    "        init_df = init_df[:-2]\n",
    "\n",
    "    dfMetrics = get_metrics(init_df, d_mode, m_name, my_token)\n",
    "    dfMetrics.to_csv(f'{pkl_file}_mmlu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDPdvxhZp7x4",
    "outputId": "d1993466-4efd-4883-96dc-6e43063bf5e8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment on meta-llama/Llama-3.2-1B                                            \n",
      "\tsaved 4.9% of lines to meta-llama-Llama-3.2-1B.pklkl\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m data_loc_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miroper_data.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m data_loc_out \u001b[38;5;241m=\u001b[39m model_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loc_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExperiments completed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[7], line 72\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(model, tokenizer, data_locs, output_dir, causal, device, token, early_exit, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m df\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m output_pdf\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#save file every line\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m percent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((index \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3000\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124msaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% of lines to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/ohpc/pub/spack/v0.21.1/opt/spack/linux-rocky8-x86_64_v3/gcc-8.5.0/py-pandas-2.1.2-223biyuu3qprro5zar6hfpslt74e63un/lib/python3.10/site-packages/pandas/core/generic.py:3085\u001b[0m, in \u001b[0;36mNDFrame.to_pickle\u001b[0;34m(self, path, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;124;03mPickle (serialize) object to file.\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m   3082\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpickle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_pickle\n\u001b[0;32m-> 3085\u001b[0m \u001b[43mto_pickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3086\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/ohpc/pub/spack/v0.21.1/opt/spack/linux-rocky8-x86_64_v3/gcc-8.5.0/py-pandas-2.1.2-223biyuu3qprro5zar6hfpslt74e63un/lib/python3.10/site-packages/pandas/io/pickle.py:105\u001b[0m, in \u001b[0;36mto_pickle\u001b[0;34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    103\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    106\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    109\u001b[0m     is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    110\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    111\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# letting pickle write directly to the buffer is more memory-efficient\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(obj, handles\u001b[38;5;241m.\u001b[39mhandle, protocol\u001b[38;5;241m=\u001b[39mprotocol)\n",
      "File \u001b[0;32m/opt/ohpc/pub/spack/v0.21.1/opt/spack/linux-rocky8-x86_64_v3/gcc-8.5.0/py-pandas-2.1.2-223biyuu3qprro5zar6hfpslt74e63un/lib/python3.10/site-packages/pandas/io/common.py:142\u001b[0m, in \u001b[0;36mIOHandles.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/ohpc/pub/spack/v0.21.1/opt/spack/linux-rocky8-x86_64_v3/gcc-8.5.0/py-pandas-2.1.2-223biyuu3qprro5zar6hfpslt74e63un/lib/python3.10/site-packages/pandas/io/common.py:134\u001b[0m, in \u001b[0;36mIOHandles.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles:\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "  for model_name in models:\n",
    "    model, tokenizer = load_model(model_name)\n",
    "    print('Running experiment on {}                                            '.format(model_name))\n",
    "\n",
    "    data_loc_in = 'iroper_data.pkl'\n",
    "    data_loc_out = model_name.replace('/','-') + '.pkl'\n",
    "\n",
    "    run_experiment(model, tokenizer, data_loc_in, data_loc_out, verbose=True)\n",
    "\n",
    "    print('Experiments completed: {}'.format(model_name))\n",
    "\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "  stop_time = time.time()\n",
    "  print('Experiment time: {}'.format(stop_time - start_time))\n",
    "  raise e\n",
    "else:\n",
    "  stop_time = time.time()\n",
    "  print('Experiment time: {}'.format(stop_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "/work/projects/bs-dcwatson43/dcwatson43/project/bin/python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
